{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==2.3.0 torchvision torchaudio -f https://download.pytorch.org/whl/cu121/torch_stable.html\n!pip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:46:17.958674Z","iopub.execute_input":"2024-09-10T08:46:17.959179Z","iopub.status.idle":"2024-09-10T08:51:33.601269Z","shell.execute_reply.started":"2024-09-10T08:46:17.959142Z","shell.execute_reply":"2024-09-10T08:51:33.600097Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: https://download.pytorch.org/whl/cu121/torch_stable.html\nCollecting torch==2.3.0\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch==2.3.0)\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0) (1.3.0)\nDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 triton-2.3.0\nCollecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-37whhr9p/unsloth_5618837d6c614c9f8a3932b6190ff598\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-37whhr9p/unsloth_5618837d6c614c9f8a3932b6190ff598\n  Resolved https://github.com/unslothai/unsloth.git to commit 6c534341bb229b136f9504443f0161645d2070c5\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes>=0.43.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.3.0+cu121)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\nCollecting xformers@ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl (164.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (21.3)\nCollecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: transformers>=4.43.2 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.44.0)\nRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.21.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (5.9.3)\nRequirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\nRequirement already satisfied: accelerate>=0.26.1 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.33.0)\nCollecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nCollecting peft!=0.11.0,>=0.7.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.24.6)\nCollecting hf-transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.6.68)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\nCollecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting torch (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting triton==2.3.1 (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.8-py3-none-any.whl size=152932 sha256=beb8dd82a2f217d1ce19b1588411133bf8d1f4c484dc70598333551d3e54c1e3\n  Stored in directory: /tmp/pip-ephem-wheel-cache-x6vsfwj3/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\nSuccessfully built unsloth\nInstalling collected packages: unsloth, triton, shtab, hf-transfer, tyro, torch, xformers, bitsandbytes, trl, peft\n  Attempting uninstall: triton\n    Found existing installation: triton 2.3.0\n    Uninstalling triton-2.3.0:\n      Successfully uninstalled triton-2.3.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.3.0+cu121\n    Uninstalling torch-2.3.0+cu121:\n      Successfully uninstalled torch-2.3.0+cu121\nSuccessfully installed bitsandbytes-0.43.3 hf-transfer-0.1.8 peft-0.12.0 shtab-1.7.1 torch-2.3.1 triton-2.3.1 trl-0.10.1 tyro-0.8.10 unsloth-2024.8 xformers-0.0.27\n","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLlamaModel\nfrom unsloth import FastLanguageModel","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:51:33.603289Z","iopub.execute_input":"2024-09-10T08:51:33.603630Z","iopub.status.idle":"2024-09-10T08:51:57.791959Z","shell.execute_reply.started":"2024-09-10T08:51:33.603595Z","shell.execute_reply":"2024-09-10T08:51:57.790616Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth.models.llama import *","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:51:57.793873Z","iopub.execute_input":"2024-09-10T08:51:57.794746Z","iopub.status.idle":"2024-09-10T08:51:57.800548Z","shell.execute_reply.started":"2024-09-10T08:51:57.794694Z","shell.execute_reply":"2024-09-10T08:51:57.799430Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:51:57.803627Z","iopub.execute_input":"2024-09-10T08:51:57.804054Z","iopub.status.idle":"2024-09-10T08:52:00.539619Z","shell.execute_reply.started":"2024-09-10T08:51:57.804002Z","shell.execute_reply":"2024-09-10T08:52:00.538402Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom typing import Optional, List, Dict  # Assuming this is the correct import\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:52:00.541361Z","iopub.execute_input":"2024-09-10T08:52:00.541720Z","iopub.status.idle":"2024-09-10T08:52:00.583868Z","shell.execute_reply.started":"2024-09-10T08:52:00.541681Z","shell.execute_reply":"2024-09-10T08:52:00.582394Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CustomEmbeddingLlamaModel(FastLanguageModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        # Additional embeddings initialized based on the parent's embedding dimensions\n        self.input_embedding = self.get_input_embeddings().to(self.device)\n        self.conversation_embedding = nn.Embedding(self.config.vocab_size, self.config.hidden_size).to(self.device)\n        self.segment_embedding = nn.Embedding(self.config.vocab_size, self.config.hidden_size).to(self.device)\n        self.emotion_embedding = nn.Embedding(self.config.vocab_size, self.config.hidden_size).to(self.device)\n        \n        # Layer normalization for the combined embeddings\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        conv_ids: Optional[torch.LongTensor] = None,\n        segment_ids: Optional[torch.LongTensor] = None,\n        emotion_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs\n    ):\n        # If inputs_embeds is not provided, compute it from input_ids\n        if inputs_embeds is None and input_ids is not None:\n            inputs_embeds = self.input_embedding(input_ids)\n        \n        # Add conversation, segment, and emotion embeddings if provided\n        if conv_ids is not None:\n            inputs_embeds += self.conversation_embedding(conv_ids)\n        if segment_ids is not None:\n            inputs_embeds += self.segment_embedding(segment_ids)\n        if emotion_ids is not None:\n            inputs_embeds += self.emotion_embedding(emotion_ids)\n        \n        # Normalize the combined embeddings\n        inputs_embeds = self.layer_norm(inputs_embeds)\n        \n        # Ensure only one of input_ids or inputs_embeds is passed to the parent's forward method\n#         if input_ids is not None:\n#             input_ids_to_pass = input_ids\n#             inputs_embeds_to_pass = None\n#         else:\n#             input_ids_to_pass = None\n#             inputs_embeds_to_pass = inputs_embeds\n        \n        # Call the parent's forward method with the correct arguments\n        return LlamaModel_fast_forward(\n            self,\n#            input_ids=input_ids_to_pass,\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            **kwargs\n        )\n\n#     def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, **kwargs):\n#         # Prepare inputs for text generation, ensuring all custom embeddings are included\n#         inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, attention_mask=attention_mask, **kwargs)\n        \n#         # Add conversation, segment, and emotion ids to inputs if they're present in kwargs\n#         for key in ['conversation_ids', 'segment_ids', 'emotion_ids']:\n#             if key in kwargs:\n#                 inputs[key] = kwargs[key]\n        \n#         return inputs\n\n#     def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder=False):\n#         # Update model kwargs based on the parent's method, ensuring custom ids are carried forward\n#         model_kwargs = super()._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder)\n        \n#         # Update conversation, segment, and emotion ids for generation\n#         for key in ['conversation_ids', 'segment_ids', 'emotion_ids']:\n#             if key in model_kwargs:\n#                 model_kwargs[key] = model_kwargs[key][:, -1].unsqueeze(-1)\n        \n#         return model_kwargs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:53:22.766344Z","iopub.execute_input":"2024-09-10T08:53:22.767115Z","iopub.status.idle":"2024-09-10T08:53:22.781865Z","shell.execute_reply.started":"2024-09-10T08:53:22.767078Z","shell.execute_reply":"2024-09-10T08:53:22.780854Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Update the model initialization\nmodel, tokenizer = CustomEmbeddingLlamaModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:53:47.853961Z","iopub.execute_input":"2024-09-10T08:53:47.854463Z","iopub.status.idle":"2024-09-10T08:54:10.232334Z","shell.execute_reply.started":"2024-09-10T08:53:47.854423Z","shell.execute_reply":"2024-09-10T08:54:10.231266Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ba917642930487693b87f1c4e7e1c57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd9dc7a447cc42fbb741f02541ee885f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601cc9c7b14a4f7595b99ebd853f42eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30cfb7be6c3e4df390f9c97b9007ba69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2bbeda02f594923ac2ef94798da7f6f"}},"metadata":{}}]},{"cell_type":"code","source":"model = CustomEmbeddingLlamaModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\"embed_tokens\", \"lm_head\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T08:54:19.764603Z","iopub.execute_input":"2024-09-10T08:54:19.765048Z","iopub.status.idle":"2024-09-10T08:54:31.693873Z","shell.execute_reply.started":"2024-09-10T08:54:19.765001Z","shell.execute_reply":"2024-09-10T08:54:31.692967Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Unsloth: Offloading input_embeddings to disk to save VRAM\nUnsloth: Offloading output_embeddings to disk to save VRAM\n","output_type":"stream"},{"name":"stderr","text":"Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Casting embed_tokens to float32\nUnsloth: Casting lm_head to float32\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"DipeshChaudhary/cleanedEmpatheticDialogueInSharegptStyle\", split = \"train\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:07:52.426235Z","iopub.execute_input":"2024-09-10T11:07:52.427090Z","iopub.status.idle":"2024-09-10T11:07:53.285158Z","shell.execute_reply.started":"2024-09-10T11:07:52.427048Z","shell.execute_reply":"2024-09-10T11:07:53.284000Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:07:54.965871Z","iopub.execute_input":"2024-09-10T11:07:54.966725Z","iopub.status.idle":"2024-09-10T11:07:54.972762Z","shell.execute_reply.started":"2024-09-10T11:07:54.966680Z","shell.execute_reply":"2024-09-10T11:07:54.971639Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"unsloth_template = (\n    \"{{ bos_token }}\"\n    \"<|start_header_id|>You are an empathetic chatbot helping user.<|end_header_id|>\\n\"\n    \"{% for message in messages %}\"\n        \"{% if message['role'] == 'user' %}\"\n            \"<conversation={{ loop.index }}/>\\n\"\n            \"<emotion={{ message['emotion'] }}/>\\n\"\n            \">>> User: {{ message['content'] }}\\n\"\n        \"{% elif message['role'] == 'assistant' %}\"\n            \"<conversation={{ loop.index }}/>\\n\"\n            \"<emotion={{ message['emotion'] }}/>\\n\"\n            \">>> Assistant: {{ message['content'] }}\\n\"\n            \"{{ eos_token }}\\n\"\n        \"{% endif %}\"\n    \"{% endfor %}\"\n    \"{% if add_generation_prompt %}\"\n        \"<conversation={{ messages|length + 1 }}/>\\n\"\n        \"<therapist/>.\\n\"\n    \"{% endif %}\"\n)\n\nunsloth_eos_token = \"eos_token\"\n# ","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:07:56.609658Z","iopub.execute_input":"2024-09-10T11:07:56.610449Z","iopub.status.idle":"2024-09-10T11:07:56.618056Z","shell.execute_reply.started":"2024-09-10T11:07:56.610411Z","shell.execute_reply":"2024-09-10T11:07:56.616771Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_chat_template(\n        tokenizer,\n        chat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token\n        mapping = {\"emotions\":\"emotion\",\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"patient\", \"assistant\" : \"therapist\"}, # ShareGPT style\n        map_eos_token = True, # Maps <|im_end|> to </s> instead\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:08:01.202690Z","iopub.execute_input":"2024-09-10T11:08:01.203486Z","iopub.status.idle":"2024-09-10T11:08:01.211535Z","shell.execute_reply.started":"2024-09-10T11:08:01.203431Z","shell.execute_reply":"2024-09-10T11:08:01.209942Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    convos = examples[\"Conversations\"]\n    texts = []\n    input_ids_list = []\n    attention_mask_list = []\n    segment_ids_list = []\n    conv_ids_list = []\n    emotion_ids_list = []\n\n    for convo in convos:\n        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n        texts.append(text)\n        \n        # Tokenize the text\n        tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=1024, return_tensors=\"pt\")\n        \n        input_ids = tokenized.input_ids.squeeze()\n        attention_mask = tokenized.attention_mask.squeeze()\n        \n        # Generate segment_ids\n        segment_ids = torch.zeros_like(input_ids)\n        for i, message in enumerate(convo):\n            start = text.index(f\">>> {'User' if message['from'] == 'patient' else 'Assistant'}: {message['value']}\")\n            end = start + len(message['value'])\n            segment_ids[start:end] = 1 if message['from'] == 'patient' else 2\n        \n        # Generate conv_ids\n        conv_ids = torch.zeros_like(input_ids)\n        for i, message in enumerate(convo):\n            start = text.index(f\">>> {'User' if message['from'] == 'patient' else 'Assistant'}: {message['value']}\")\n            end = start + len(message['value'])\n            conv_ids[start:end] = i + 1\n        \n        # Generate emotion_ids\n        emotion_ids = torch.zeros_like(input_ids)\n        emotion_mapping = {\n                \"afraid\": 1, \"angry\": 2, \"annoyed\": 3, \"anticipating\": 4, \"anxious\": 5,\n                \"apprehensive\": 6, \"ashamed\": 7, \"caring\": 8, \"confident\": 9, \"content\": 10,\n                \"devastated\": 11, \"disappointed\": 12, \"disgusted\": 13, \"embarrassed\": 14,\n                \"excited\": 15, \"faithful\": 16, \"furious\": 17, \"grateful\": 18, \"guilty\": 19,\n                \"hopeful\": 20, \"impressed\": 21, \"jealous\": 22, \"joyful\": 23, \"lonely\": 24,\n                \"nostalgic\": 25, \"prepared\": 26, \"proud\": 27, \"sad\": 28, \"sentimental\": 29,\n                \"surprised\": 30, \"terrified\": 31, \"trusting\": 32, \"agreeing\": 33, \"acknowledging\": 34,\n                \"encouraging\": 35, \"consoling\": 36, \"sympathizing\": 37, \"suggesting\": 38,\n                \"questioning\": 39, \"wishing\": 40, \"neutral\": 41}\n        for i, message in enumerate(convo):\n            start = text.index(f\">>> {'User' if message['from'] == 'patient' else 'Assistant'}: {message['value']}\")\n            end = start + len(message['value'])\n            emotion_ids[start:end] = emotion_mapping.get(message['emotion'], 0)\n        \n        input_ids_list.append(input_ids)\n        attention_mask_list.append(attention_mask)\n        segment_ids_list.append(segment_ids)\n        conv_ids_list.append(conv_ids)\n        emotion_ids_list.append(emotion_ids)\n\n    return {\n        \"text\": texts,\n        \"input_ids\": input_ids_list,\n        \"attention_mask\": attention_mask_list,\n        \"segment_ids\": segment_ids_list,\n        \"conv_ids\": conv_ids_list,\n        \"emotion_ids\": emotion_ids_list,\n    }        \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:08:02.869919Z","iopub.execute_input":"2024-09-10T11:08:02.870315Z","iopub.status.idle":"2024-09-10T11:08:02.889511Z","shell.execute_reply.started":"2024-09-10T11:08:02.870278Z","shell.execute_reply":"2024-09-10T11:08:02.888346Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:08:04.578883Z","iopub.execute_input":"2024-09-10T11:08:04.579612Z","iopub.status.idle":"2024-09-10T11:08:46.475350Z","shell.execute_reply.started":"2024-09-10T11:08:04.579574Z","shell.execute_reply":"2024-09-10T11:08:46.474284Z"},"trusted":true},"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19533 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8db0feb8808849168c6235f5546de416"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:34:39.190385Z","iopub.execute_input":"2024-09-10T10:34:39.191113Z","iopub.status.idle":"2024-09-10T10:34:39.200074Z","shell.execute_reply.started":"2024-09-10T10:34:39.191069Z","shell.execute_reply":"2024-09-10T10:34:39.198687Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask', 'segment_ids', 'conv_ids', 'emotion_ids'],\n    num_rows: 19533\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:52:25.356732Z","iopub.execute_input":"2024-09-10T09:52:25.357135Z","iopub.status.idle":"2024-09-10T09:52:25.368183Z","shell.execute_reply.started":"2024-09-10T09:52:25.357097Z","shell.execute_reply":"2024-09-10T09:52:25.367096Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"PreTrainedTokenizerFast(name_or_path='unsloth/llama-3-8b-Instruct-bnb-4bit', vocab_size=128000, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '<|reserved_special_token_250|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128004: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128005: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128008: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128010: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128011: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128012: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128013: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128014: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128015: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128016: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128017: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128018: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128019: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128020: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128021: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128022: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128023: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128024: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128025: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128026: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128027: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128028: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128029: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128030: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128031: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128032: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128033: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128034: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128035: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128036: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128037: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128038: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128039: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128040: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128041: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128042: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128043: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128044: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128045: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128046: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128047: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128048: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128049: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128050: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128051: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128052: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128053: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128054: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128055: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128056: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128057: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128058: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128059: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128060: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128061: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128062: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128063: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128064: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128065: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128066: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128067: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128068: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128069: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128070: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128071: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128072: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128073: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128074: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128075: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128076: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128077: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128078: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128079: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128080: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128081: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128082: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128083: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128084: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128085: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128086: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128087: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128088: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128089: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128090: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128091: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128092: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128093: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128094: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128095: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128096: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128097: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128098: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128099: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128100: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128101: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128102: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128103: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128104: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128105: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128106: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128107: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128108: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128109: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128110: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128111: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128112: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128113: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128114: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128115: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128116: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128117: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128118: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128119: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128120: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128121: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128122: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128123: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128124: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128125: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128126: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128127: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128128: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128129: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128130: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128131: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128132: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128133: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128134: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128135: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128136: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128137: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128138: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128139: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128140: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128141: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128142: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128143: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128144: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128145: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128146: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128147: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128148: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128149: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128150: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128151: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128152: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128153: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128154: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128155: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128156: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128157: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128158: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128159: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128160: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128161: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128162: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128163: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128164: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128165: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128166: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128167: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128168: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128169: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128170: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128171: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128172: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128173: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128174: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128175: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128176: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128177: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128178: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128179: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128180: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128181: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128182: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128183: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128184: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128185: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128186: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128187: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128188: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128189: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128190: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128191: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128192: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128193: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128194: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128195: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128196: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128197: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128198: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128199: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128200: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128201: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128202: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128203: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128204: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128205: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128206: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128207: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128208: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128209: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128210: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128211: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128212: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128213: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128214: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128215: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128216: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128217: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128218: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128219: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128220: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128221: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128222: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128223: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128224: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128225: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128226: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128227: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128228: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128229: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128230: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128231: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128232: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128233: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128234: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128235: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128236: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128237: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128238: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128239: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128240: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128241: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128242: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128243: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128244: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128245: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128246: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128247: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128248: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128249: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128250: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128251: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128252: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128253: AddedToken(\"<|reserved_special_token_248|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128254: AddedToken(\"<|reserved_special_token_249|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t128255: AddedToken(\"<|reserved_special_token_250|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"length1 =len(dataset[0]['emotion_ids'])\nprint(length1)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T09:59:32.383349Z","iopub.execute_input":"2024-09-10T09:59:32.383736Z","iopub.status.idle":"2024-09-10T09:59:32.402946Z","shell.execute_reply.started":"2024-09-10T09:59:32.383701Z","shell.execute_reply":"2024-09-10T09:59:32.401857Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"2048\n","output_type":"stream"}]},{"cell_type":"code","source":"length=len(dataset[0]['segment_ids'])\nprint(length)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:59:21.691569Z","iopub.execute_input":"2024-09-10T10:59:21.691965Z","iopub.status.idle":"2024-09-10T10:59:21.706558Z","shell.execute_reply.started":"2024-09-10T10:59:21.691929Z","shell.execute_reply":"2024-09-10T10:59:21.705365Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"2048\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.encode(dataset[0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-10T10:11:21.668838Z","iopub.execute_input":"2024-09-10T10:11:21.669278Z","iopub.status.idle":"2024-09-10T10:11:21.697296Z","shell.execute_reply.started":"2024-09-10T10:11:21.669232Z","shell.execute_reply":"2024-09-10T10:11:21.696074Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"[128000,\n 128000,\n 128006,\n 2675,\n 527,\n 459,\n 36681,\n 5411,\n 6369,\n 6465,\n 10695,\n 1217,\n 13,\n 128007,\n 198,\n 27,\n 62737,\n 28,\n 16,\n 10381,\n 27,\n 74453,\n 28,\n 12490,\n 989,\n 10381,\n 20761,\n 2724,\n 25,\n 358,\n 6227,\n 2133,\n 311,\n 1518,\n 279,\n 52933,\n 449,\n 856,\n 1888,\n 4333,\n 13,\n 1102,\n 574,\n 279,\n 1176,\n 892,\n 584,\n 3596,\n 7543,\n 892,\n 7636,\n 3871,\n 13,\n 10541,\n 1070,\n 574,\n 264,\n 2763,\n 315,\n 1274,\n 11,\n 584,\n 6612,\n 1093,\n 279,\n 1193,\n 1274,\n 304,\n 279,\n 1917,\n 627,\n 27,\n 62737,\n 28,\n 17,\n 10381,\n 27,\n 74453,\n 28,\n 7998,\n 287,\n 10381,\n 20761,\n 22103,\n 25,\n 15148,\n 420,\n 264,\n 4333,\n 499,\n 1051,\n 304,\n 3021,\n 449,\n 11,\n 477,\n 1120,\n 264,\n 1888,\n 4333,\n 5380,\n 128009,\n 198,\n 27,\n 62737,\n 28,\n 18,\n 10381,\n 27,\n 74453,\n 22495,\n 537,\n 24823,\n 292,\n 10381,\n 20761,\n 2724,\n 25,\n 1115,\n 574,\n 264,\n 1888,\n 4333,\n 13,\n 358,\n 3194,\n 1077,\n 627,\n 27,\n 62737,\n 28,\n 19,\n 10381,\n 27,\n 74453,\n 28,\n 7998,\n 287,\n 10381,\n 20761,\n 22103,\n 25,\n 11208,\n 706,\n 1364,\n 8208,\n 5380,\n 128009,\n 198,\n 27,\n 62737,\n 28,\n 20,\n 10381,\n 27,\n 74453,\n 28,\n 12490,\n 989,\n 10381,\n 20761,\n 2724,\n 25,\n 1226,\n 912,\n 5129,\n 3137,\n 627,\n 27,\n 62737,\n 28,\n 21,\n 10381,\n 27,\n 74453,\n 28,\n 7998,\n 287,\n 10381,\n 20761,\n 22103,\n 25,\n 8840,\n 574,\n 420,\n 2555,\n 430,\n 7077,\n 1606,\n 315,\n 459,\n 5811,\n 5380,\n 128009,\n 198]"},"metadata":{}}]},{"cell_type":"markdown","source":"# **SFTTTRAINER**","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:09:12.868954Z","iopub.execute_input":"2024-09-10T11:09:12.869392Z","iopub.status.idle":"2024-09-10T11:09:12.876630Z","shell.execute_reply.started":"2024-09-10T11:09:12.869349Z","shell.execute_reply":"2024-09-10T11:09:12.875310Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\nimport torch\n\nclass CustomDataCollator(DataCollatorForLanguageModeling):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        \n        # Add custom ids to the batch, if they exist\n        custom_fields = ['input_ids', 'segment_ids', 'conv_ids', 'emotion_ids']\n        for field in custom_fields:\n            if all(field in f for f in features):\n                # Convert to tensor if it's a list\n                field_data = [f[field] for f in features]\n                if isinstance(field_data[0], list):\n                    batch[field] = torch.tensor(field_data)\n                else:\n                    batch[field] = torch.stack(field_data)\n            else:\n                print(f\"Warning: '{field}' not found in all features. Skipping this field.\")\n        \n        return batch\n\n# Initialize the custom data collator\ndata_collator = CustomDataCollator(tokenizer=tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:09:16.508797Z","iopub.execute_input":"2024-09-10T11:09:16.509199Z","iopub.status.idle":"2024-09-10T11:09:16.519894Z","shell.execute_reply.started":"2024-09-10T11:09:16.509162Z","shell.execute_reply":"2024-09-10T11:09:16.518739Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n            per_device_train_batch_size = 2,\n            gradient_accumulation_steps = 4,\n            warmup_steps = 5,\n            max_steps = 60,\n            learning_rate = 2e-4,\n            fp16 = not is_bfloat16_supported(),\n            bf16 = is_bfloat16_supported(),\n            logging_steps = 1,\n            optim = \"adamw_8bit\",\n            weight_decay = 0.01,\n            lr_scheduler_type = \"linear\",\n            seed = 3407,\n            output_dir = \"outputs\",\n        # ... (keep the existing arguments)\n    ),\n    data_collator=data_collator, \n)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:10:41.477045Z","iopub.execute_input":"2024-09-10T11:10:41.477498Z","iopub.status.idle":"2024-09-10T11:10:41.562675Z","shell.execute_reply.started":"2024-09-10T11:10:41.477444Z","shell.execute_reply":"2024-09-10T11:10:41.561705Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"# Run the training\ntrainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T11:10:44.404836Z","iopub.execute_input":"2024-09-10T11:10:44.405299Z","iopub.status.idle":"2024-09-10T11:10:58.612266Z","shell.execute_reply.started":"2024-09-10T11:10:44.405259Z","shell.execute_reply":"2024-09-10T11:10:58.610667Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 19,533 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 1,092,616,192\n","output_type":"stream"},{"name":"stdout","text":"Warning: 'segment_ids' not found in all features. Skipping this field.\nWarning: 'conv_ids' not found in all features. Skipping this field.\nWarning: 'emotion_ids' not found in all features. Skipping this field.\nWarning: 'segment_ids' not found in all features. Skipping this field.\nWarning: 'conv_ids' not found in all features. Skipping this field.\nWarning: 'emotion_ids' not found in all features. Skipping this field.\nWarning: 'segment_ids' not found in all features. Skipping this field.\nWarning: 'conv_ids' not found in all features. Skipping this field.\nWarning: 'emotion_ids' not found in all features. Skipping this field.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<string>:145\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n","File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3359\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3357\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2155\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU "],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.96 GiB. GPU ","output_type":"error"}]},{"cell_type":"code","source":"# from torch.utils.data import Dataset\n# import torch\n\n# class PreTokenizedDataset(Dataset):\n#     def __init__(self, dataset):\n#         self.dataset = dataset\n\n#     def __len__(self):\n#         return len(self.dataset)\n\n#     def __getitem__(self, idx):\n#         item = self.dataset[idx]\n#         return {\n#             \"input_ids\": item[\"input_ids\"],\n#             \"attention_mask\": item[\"attention_mask\"],\n#             \"labels\": item[\"input_ids\"],  # For language modeling, labels are typically the same as input_ids\n#             \"segment_ids\": item[\"segment_ids\"],\n#             \"conv_ids\": item[\"conv_ids\"],\n#             \"emotion_ids\": item[\"emotion_ids\"],\n#         }\n\n# # Create an instance of the pre-tokenized dataset\n# pre_tokenized_dataset = PreTokenizedDataset(dataset)\n\n# # Update the CustomDataCollator\n# class CustomDataCollator(DataCollatorForLanguageModeling):\n#     def __call__(self, features):\n#         batch = super().__call__(features)\n        \n#         # Add custom ids to the batch\n#         custom_fields = ['segment_ids', 'conv_ids', 'emotion_ids']\n#         for field in custom_fields:\n#             if all(field in f for f in features):\n#                 batch[field] = torch.stack([f[field] for f in features])\n#             else:\n#                 print(f\"Warning: '{field}' not found in all features. Skipping this field.\")\n        \n#         return batch\n\n# # Initialize the custom data collator\n# data_collator = CustomDataCollator(tokenizer=tokenizer, mlm=False)\n\n# # Update the SFTTrainer initialization\n# trainer = SFTTrainer(\n#     model=model,\n#     tokenizer=tokenizer,\n#     train_dataset=pre_tokenized_dataset,\n#     dataset_text_field=None,  # Set this to None since we're using a pre-tokenized dataset\n#     max_seq_length=max_seq_length,\n#     dataset_num_proc=2,\n#     packing=False,\n#     args=TrainingArguments(\n#         # ... (keep existing arguments)\n#     ),\n#     data_collator=data_collator,\n# )\n\n# # Run the training\n# trainer_stats = trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}