{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-11T08:58:39.486265Z","iopub.status.busy":"2024-09-11T08:58:39.485888Z","iopub.status.idle":"2024-09-11T09:04:14.100900Z","shell.execute_reply":"2024-09-11T09:04:14.099553Z","shell.execute_reply.started":"2024-09-11T08:58:39.486223Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/cu121/torch_stable.html\n","Collecting torch==2.3.0\n","  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\n","Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.15.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (1.13.2)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n","  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.3.0 (from torch==2.3.0)\n","  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n","  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0) (1.3.0)\n","Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.4.0\n","    Uninstalling torch-2.4.0:\n","      Successfully uninstalled torch-2.4.0\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 triton-2.3.0\n","Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-9g6e9eia/unsloth_f91dd0ef3d964e71a9ce2abc76030438\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-9g6e9eia/unsloth_f91dd0ef3d964e71a9ce2abc76030438\n","  Resolved https://github.com/unslothai/unsloth.git to commit 6c534341bb229b136f9504443f0161645d2070c5\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting bitsandbytes>=0.43.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.3.0+cu121)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n","Collecting xformers@ https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.27-cp310-cp310-manylinux2014_x86_64.whl (164.1 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m164.1/164.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (21.3)\n","Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\n","Requirement already satisfied: transformers>=4.43.2 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.44.0)\n","Requirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.21.0)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (5.9.3)\n","Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n","Requirement already satisfied: accelerate>=0.26.1 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.33.0)\n","Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\n","Collecting peft!=0.11.0,>=0.7.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.24.6)\n","Collecting hf-transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.4.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.15.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.6.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.1.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.13.2)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.6.68)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n","Requirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n","Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting torch (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting triton==2.3.1 (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n","  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.7.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n","Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n","\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: unsloth\n","  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.8-py3-none-any.whl size=152932 sha256=f23cd5bed6c70753d9796eaae0c340ab595d637f025d72b9eeaa6f536cc0e9fd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6zflz7ba/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n","Successfully built unsloth\n","Installing collected packages: unsloth, triton, shtab, hf-transfer, tyro, torch, xformers, bitsandbytes, trl, peft\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.3.0\n","    Uninstalling triton-2.3.0:\n","      Successfully uninstalled triton-2.3.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.3.0+cu121\n","    Uninstalling torch-2.3.0+cu121:\n","      Successfully uninstalled torch-2.3.0+cu121\n","Successfully installed bitsandbytes-0.43.3 hf-transfer-0.1.8 peft-0.12.0 shtab-1.7.1 torch-2.3.1 triton-2.3.1 trl-0.10.1 tyro-0.8.10 unsloth-2024.8 xformers-0.0.27\n"]}],"source":["!pip install torch==2.3.0 torchvision torchaudio -f https://download.pytorch.org/whl/cu121/torch_stable.html\n","!pip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:04:48.204835Z","iopub.status.busy":"2024-09-11T09:04:48.204404Z","iopub.status.idle":"2024-09-11T09:05:09.821915Z","shell.execute_reply":"2024-09-11T09:05:09.820653Z","shell.execute_reply.started":"2024-09-11T09:04:48.204790Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]}],"source":["from unsloth import FastLlamaModel\n","from unsloth import FastLanguageModel"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:09.827003Z","iopub.status.busy":"2024-09-11T09:05:09.825849Z","iopub.status.idle":"2024-09-11T09:05:09.831792Z","shell.execute_reply":"2024-09-11T09:05:09.830855Z","shell.execute_reply.started":"2024-09-11T09:05:09.826960Z"},"trusted":true},"outputs":[],"source":["from unsloth.models.llama import *"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:09.833496Z","iopub.status.busy":"2024-09-11T09:05:09.833112Z","iopub.status.idle":"2024-09-11T09:05:09.842879Z","shell.execute_reply":"2024-09-11T09:05:09.841998Z","shell.execute_reply.started":"2024-09-11T09:05:09.833450Z"},"trusted":true},"outputs":[],"source":["import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:09.844797Z","iopub.status.busy":"2024-09-11T09:05:09.844113Z","iopub.status.idle":"2024-09-11T09:05:09.851909Z","shell.execute_reply":"2024-09-11T09:05:09.850975Z","shell.execute_reply.started":"2024-09-11T09:05:09.844754Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","from typing import Optional, List, Dict  # Assuming this is the correct import\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:09.854013Z","iopub.status.busy":"2024-09-11T09:05:09.853678Z","iopub.status.idle":"2024-09-11T09:05:09.869828Z","shell.execute_reply":"2024-09-11T09:05:09.868868Z","shell.execute_reply.started":"2024-09-11T09:05:09.853982Z"},"trusted":true},"outputs":[],"source":["class CustomEmbeddingLlamaModel(FastLanguageModel):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        num_emotions=41\n","        # Additional embeddings initialized based on the parent's embedding dimensions\n","        self.input_embedding = self.get_input_embeddings().to(self.device)\n","        self.emotion_embedding = nn.Embedding(num_emotions , self.config.hidden_size).to(self.device)\n","        \n","        # Layer normalization for the combined embeddings\n","        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        conv_ids: Optional[torch.LongTensor] = None,\n","        segment_ids: Optional[torch.LongTensor] = None,\n","        emotion_ids: Optional[torch.LongTensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        past_key_values: Optional[List[torch.FloatTensor]] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","        **kwargs\n","    ):\n","        # If inputs_embeds is not provided, compute it from input_ids\n","        if inputs_embeds is None and input_ids is not None:\n","            inputs_embeds = self.input_embedding(input_ids)\n","        \n","        if emotion_ids is not None:\n","            inputs_embeds += self.emotion_embedding(emotion_ids)\n","        \n","        # Normalize the combined embeddings\n","        inputs_embeds = self.layer_norm(inputs_embeds)\n","        \n","        # Ensure only one of input_ids or inputs_embeds is passed to the parent's forward method\n","#         if input_ids is not None:\n","#             input_ids_to_pass = input_ids\n","#             inputs_embeds_to_pass = None\n","#         else:\n","#             input_ids_to_pass = None\n","#             inputs_embeds_to_pass = inputs_embeds\n","        \n","        # Call the parent's forward method with the correct arguments\n","        return LlamaModel_fast_forward(\n","            self,\n","#            input_ids=input_ids_to_pass,\n","            inputs_embeds=inputs_embeds,\n","            attention_mask=attention_mask,\n","            position_ids=position_ids,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","            **kwargs\n","        )\n","\n","#     def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, **kwargs):\n","#         # Prepare inputs for text generation, ensuring all custom embeddings are included\n","#         inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, attention_mask=attention_mask, **kwargs)\n","        \n","#         # Add conversation, segment, and emotion ids to inputs if they're present in kwargs\n","#         for key in ['conversation_ids', 'segment_ids', 'emotion_ids']:\n","#             if key in kwargs:\n","#                 inputs[key] = kwargs[key]\n","        \n","#         return inputs\n","\n","#     def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder=False):\n","#         # Update model kwargs based on the parent's method, ensuring custom ids are carried forward\n","#         model_kwargs = super()._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder)\n","        \n","#         # Update conversation, segment, and emotion ids for generation\n","#         for key in ['conversation_ids', 'segment_ids', 'emotion_ids']:\n","#             if key in model_kwargs:\n","#                 model_kwargs[key] = model_kwargs[key][:, -1].unsqueeze(-1)\n","        \n","#         return model_kwargs\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:09.872040Z","iopub.status.busy":"2024-09-11T09:05:09.871411Z","iopub.status.idle":"2024-09-11T09:05:32.394387Z","shell.execute_reply":"2024-09-11T09:05:32.393054Z","shell.execute_reply.started":"2024-09-11T09:05:09.871997Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71b45c6da21e45e89c64b29b62a15073","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1ac5d477dbf4eb3968b0ad38929a6db","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82d6f0f090fe43bcbaa67dec7ae884e3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70900bf1beac4c8ab9e0ce243986cf82","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"720e0c68abcb4bdbbad9f63d29ac1825","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Update the model initialization\n","model, tokenizer = CustomEmbeddingLlamaModel.from_pretrained(\n","    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n","    max_seq_length=max_seq_length,\n","    dtype=dtype,\n","    load_in_4bit=load_in_4bit,\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:32.396351Z","iopub.status.busy":"2024-09-11T09:05:32.395919Z","iopub.status.idle":"2024-09-11T09:05:45.009288Z","shell.execute_reply":"2024-09-11T09:05:45.008360Z","shell.execute_reply.started":"2024-09-11T09:05:32.396300Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unsloth: Offloading input_embeddings to disk to save VRAM\n","Unsloth: Offloading output_embeddings to disk to save VRAM\n"]},{"name":"stderr","output_type":"stream","text":["Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n","are not enabled or a bias term (like in Qwen) is used.\n","Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n","are not enabled or a bias term (like in Qwen) is used.\n","Unsloth 2024.8 patched 32 layers with 32 QKV layers, 0 O layers and 0 MLP layers.\n"]},{"name":"stdout","output_type":"stream","text":["Unsloth: Casting embed_tokens to float32\n","Unsloth: Casting lm_head to float32\n"]}],"source":["model = CustomEmbeddingLlamaModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",\"embed_tokens\", \"lm_head\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:45.013273Z","iopub.status.busy":"2024-09-11T09:05:45.012837Z","iopub.status.idle":"2024-09-11T09:05:52.902503Z","shell.execute_reply":"2024-09-11T09:05:52.901752Z","shell.execute_reply.started":"2024-09-11T09:05:45.013223Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd29f43b971a4050865752ec9429c571","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/391 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ce5811f358d4ca2b33e3ae6bb1db80a","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/3.98M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"900842c4103d4d3eb1a4498fd29556ea","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/19533 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"DipeshChaudhary/cleanedEmpatheticDialogueInSharegptStyle\", split = \"train\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:52.903932Z","iopub.status.busy":"2024-09-11T09:05:52.903640Z","iopub.status.idle":"2024-09-11T09:05:52.908697Z","shell.execute_reply":"2024-09-11T09:05:52.907490Z","shell.execute_reply.started":"2024-09-11T09:05:52.903900Z"},"trusted":true},"outputs":[],"source":["from unsloth.chat_templates import get_chat_template"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:52.910672Z","iopub.status.busy":"2024-09-11T09:05:52.910213Z","iopub.status.idle":"2024-09-11T09:05:54.590684Z","shell.execute_reply":"2024-09-11T09:05:54.589604Z","shell.execute_reply.started":"2024-09-11T09:05:52.910625Z"},"trusted":true},"outputs":[],"source":["unsloth_template = (\n","    \"{% for message in messages %}\"\n","        \"{{ bos_token }}\"\n","        \"{% if message['role'] == 'user' %}\"\n","            \">>> User: {{ message['content'] }}\\n\"\n","        \"{% elif message['role'] == 'assistant' %}\"\n","            \">>> Assistant: {{ message['content'] }}\\n\"\n","        \"{% endif %}\"\n","        \"{{ eos_token }}\\n\"\n","    \"{% endfor %}\"\n","    \"{% if add_generation_prompt %}\"\n","        \">>> Assistant: \\n\"\n","    \"{% endif %}\"\n",")\n","\n","unsloth_eos_token = \"eos_token\"\n","# "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:05:54.592535Z","iopub.status.busy":"2024-09-11T09:05:54.592090Z","iopub.status.idle":"2024-09-11T09:05:54.601404Z","shell.execute_reply":"2024-09-11T09:05:54.600433Z","shell.execute_reply.started":"2024-09-11T09:05:54.592491Z"},"trusted":true},"outputs":[],"source":["tokenizer = get_chat_template(\n","        tokenizer,\n","        chat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token\n","        mapping = {\"emotions\":\"emotion\",\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"patient\", \"assistant\" : \"therapist\"}, # ShareGPT style\n","        map_eos_token = True, # Maps <|im_end|> to </s> instead\n","    )\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:08:35.921452Z","iopub.status.busy":"2024-09-11T09:08:35.921007Z","iopub.status.idle":"2024-09-11T09:08:35.938717Z","shell.execute_reply":"2024-09-11T09:08:35.937638Z","shell.execute_reply.started":"2024-09-11T09:08:35.921407Z"},"trusted":true},"outputs":[],"source":["def formatting_prompts_func(examples):\n","    convos = examples[\"Conversations\"]\n","    texts = []\n","    input_ids_list = []\n","    attention_mask_list = []\n","    segment_ids_list = []\n","#     conv_ids_list = []\n","    emotion_ids_list = []\n","    con=0\n","    for convo in convos:\n","        con=+1\n","        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n","        texts.append(text)\n","        \n","        # Tokenize the text\n","        tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n","        \n","        input_ids = tokenized.input_ids.squeeze()\n","        attention_mask = tokenized.attention_mask.squeeze()\n","        \n","        # Generate segment_ids\n","\n","        # Initialize segment_ids with zeros\n","        segment_ids = torch.zeros_like(input_ids)\n","\n","        # Define the values that should map to 0\n","        zero_values = torch.tensor([128255, 128009, 198, 128000])\n","        # Initialize state variables\n","        current_segment = 0  # Start with segment 0\n","        count_128000 = 0     # Counter to track how many times 128000 is seen\n","        current_emotion=0\n","        # Iterate over input_ids to set segment_ids based on the corresponding tokens\n","                # Generate emotion_ids\n","        emotion_ids = torch.zeros_like(input_ids)\n","        emotion_mapping = {\n","                \"afraid\": 1, \"angry\": 2, \"annoyed\": 3, \"anticipating\": 4, \"anxious\": 5,\n","                \"apprehensive\": 6, \"ashamed\": 7, \"caring\": 8, \"confident\": 9, \"content\": 10,\n","                \"devastated\": 11, \"disappointed\": 12, \"disgusted\": 13, \"embarrassed\": 14,\n","                \"excited\": 15, \"faithful\": 16, \"furious\": 17, \"grateful\": 18, \"guilty\": 19,\n","                \"hopeful\": 20, \"impressed\": 21, \"jealous\": 22, \"joyful\": 23, \"lonely\": 24,\n","                \"nostalgic\": 25, \"prepared\": 26, \"proud\": 27, \"sad\": 28, \"sentimental\": 29,\n","                \"surprised\": 30, \"terrified\": 31, \"trusting\": 32, \"agreeing\": 33, \"acknowledging\": 34,\n","                \"encouraging\": 35, \"consoling\": 36, \"sympathizing\": 37, \"suggesting\": 38,\n","                \"questioning\": 39, \"wishing\": 40, \"neutral\": 41}\n","        msg_emotion=[]\n","        for i, message in enumerate(convo):\n","            msg_emotion.append(emotion_mapping.get(message['emotion']))\n","        emo_idx=0\n","        for idx, token in enumerate(input_ids):\n","            if token in zero_values:\n","                segment_ids[idx] = 0\n","                emotion_ids[idx] =0\n","                if token == 128000:\n","                    count_128000 += 1\n","                    if count_128000 >1:\n","                        current_emotion=msg_emotion[emo_idx]\n","                        emo_idx=+1\n","                    # Toggle between segment 1 and 2 depending on the occurrence of 128000\n","                    if count_128000 % 2 == 0:\n","                        current_segment = 1\n","\n","                    else:\n","                        current_segment = 2\n","            else:\n","                # For non-zero_values, assign the current_segment\n","                segment_ids[idx] = current_segment\n","                emotion_ids[idx]=current_emotion\n","\n","\n","#         segment_ids = torch.zeros_like(input_ids)\n","#         for i, message in enumerate(convo):\n","#             start = text.index(f\">>> {'User' if message['from'] == 'patient' else 'Assistant'}: {message['value']}\")\n","#             end = start + len(message['value'])\n","#             segment_ids[start:end] = 1 if message['from'] == 'patient' else 2\n","        \n","#         # Generate conv_ids\n","#         conv_ids = torch.zeros_like(input_ids)\n","#         for i, message in enumerate(convo):\n","#             start = text.index(f\">>> {'User' if message['from'] == 'patient' else 'Assistant'}: {message['value']}\")\n","#             end = start + len(message['value'])\n","#             conv_ids[start:end] = i + 1\n","        \n","\n","            \n","        input_ids_list.append(input_ids)\n","        attention_mask_list.append(attention_mask)\n","        segment_ids_list.append(segment_ids)\n","#         conv_ids_list.append(conv_ids)\n","        emotion_ids_list.append(emotion_ids)\n","\n","    return {\n","        \"text\": texts,\n","        \"input_ids\": input_ids_list,\n","        \"attention_mask\": attention_mask_list,\n","        \"segment_ids\": segment_ids_list,\n","#         \"conv_ids\": conv_ids_list,\n","        \"emotion_ids\": emotion_ids_list,\n","    }        \n","    \n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:08:37.143289Z","iopub.status.busy":"2024-09-11T09:08:37.142516Z","iopub.status.idle":"2024-09-11T09:13:19.008426Z","shell.execute_reply":"2024-09-11T09:13:19.007449Z","shell.execute_reply.started":"2024-09-11T09:08:37.143245Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49617a3d3e1943e891263bb1fa4a770b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/19533 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:15:12.374164Z","iopub.status.busy":"2024-09-11T09:15:12.373731Z","iopub.status.idle":"2024-09-11T09:15:12.380908Z","shell.execute_reply":"2024-09-11T09:15:12.379950Z","shell.execute_reply.started":"2024-09-11T09:15:12.374127Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text', 'input_ids', 'attention_mask', 'segment_ids', 'emotion_ids'],\n","    num_rows: 19533\n","})"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:13:19.104454Z","iopub.status.busy":"2024-09-11T09:13:19.103640Z","iopub.status.idle":"2024-09-11T09:13:19.112024Z","shell.execute_reply":"2024-09-11T09:13:19.111149Z","shell.execute_reply.started":"2024-09-11T09:13:19.104418Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'<|begin_of_text|>>>> User: I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\\n<|eot_id|>\\n<|begin_of_text|>>>> Assistant: Was this a friend you were in love with, or just a best friend?\\n<|eot_id|>\\n<|begin_of_text|>>>> User: This was a best friend. I miss her.\\n<|eot_id|>\\n<|begin_of_text|>>>> Assistant: Where has she gone?\\n<|eot_id|>\\n<|begin_of_text|>>>> User: We no longer talk.\\n<|eot_id|>\\n<|begin_of_text|>>>> Assistant: Oh was this something that happened because of an argument?\\n<|eot_id|>\\n'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["dataset[0]['text']"]},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-11T09:13:19.011163Z","iopub.status.busy":"2024-09-11T09:13:19.010469Z","iopub.status.idle":"2024-09-11T09:13:19.028557Z","shell.execute_reply":"2024-09-11T09:13:19.027670Z","shell.execute_reply.started":"2024-09-11T09:13:19.011115Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["[128000,\n"," 128000,\n"," 20761,\n"," 2724,\n"," 25,\n"," 358,\n"," 6227,\n"," 2133,\n"," 311,\n"," 1518,\n"," 279,\n"," 52933,\n"," 449,\n"," 856,\n"," 1888,\n"," 4333,\n"," 13,\n"," 1102,\n"," 574,\n"," 279,\n"," 1176,\n"," 892,\n"," 584,\n"," 3596,\n"," 7543,\n"," 892,\n"," 7636,\n"," 3871,\n"," 13,\n"," 10541,\n"," 1070,\n"," 574,\n"," 264,\n"," 2763,\n"," 315,\n"," 1274,\n"," 11,\n"," 584,\n"," 6612,\n"," 1093,\n"," 279,\n"," 1193,\n"," 1274,\n"," 304,\n"," 279,\n"," 1917,\n"," 627,\n"," 128009,\n"," 198,\n"," 128000,\n"," 20761,\n"," 22103,\n"," 25,\n"," 15148,\n"," 420,\n"," 264,\n"," 4333,\n"," 499,\n"," 1051,\n"," 304,\n"," 3021,\n"," 449,\n"," 11,\n"," 477,\n"," 1120,\n"," 264,\n"," 1888,\n"," 4333,\n"," 5380,\n"," 128009,\n"," 198,\n"," 128000,\n"," 20761,\n"," 2724,\n"," 25,\n"," 1115,\n"," 574,\n"," 264,\n"," 1888,\n"," 4333,\n"," 13,\n"," 358,\n"," 3194,\n"," 1077,\n"," 627,\n"," 128009,\n"," 198,\n"," 128000,\n"," 20761,\n"," 22103,\n"," 25,\n"," 11208,\n"," 706,\n"," 1364,\n"," 8208,\n"," 5380,\n"," 128009,\n"," 198,\n"," 128000,\n"," 20761,\n"," 2724,\n"," 25,\n"," 1226,\n"," 912,\n"," 5129,\n"," 3137,\n"," 627,\n"," 128009,\n"," 198,\n"," 128000,\n"," 20761,\n"," 22103,\n"," 25,\n"," 8840,\n"," 574,\n"," 420,\n"," 2555,\n"," 430,\n"," 7077,\n"," 1606,\n"," 315,\n"," 459,\n"," 5811,\n"," 5380,\n"," 128009,\n"," 198,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255,\n"," 128255]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["dataset[0]['input_ids']"]},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-11T09:13:19.030357Z","iopub.status.busy":"2024-09-11T09:13:19.029989Z","iopub.status.idle":"2024-09-11T09:13:19.100890Z","shell.execute_reply":"2024-09-11T09:13:19.099956Z","shell.execute_reply.started":"2024-09-11T09:13:19.030325Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["[0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["dataset[0]['segment_ids']"]},{"cell_type":"code","execution_count":22,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-11T09:13:53.474701Z","iopub.status.busy":"2024-09-11T09:13:53.474225Z","iopub.status.idle":"2024-09-11T09:13:53.491641Z","shell.execute_reply":"2024-09-11T09:13:53.490588Z","shell.execute_reply.started":"2024-09-11T09:13:53.474659Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["[0,\n"," 0,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 24,\n"," 0,\n"," 0,\n"," 0,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 0,\n"," 0,\n"," 0,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 0,\n"," 0,\n"," 0,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 0,\n"," 0,\n"," 0,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 0,\n"," 0,\n"," 0,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 39,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["dataset[0]['emotion_ids']"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:16:26.744265Z","iopub.status.busy":"2024-09-11T09:16:26.743445Z","iopub.status.idle":"2024-09-11T09:16:26.750877Z","shell.execute_reply":"2024-09-11T09:16:26.749849Z","shell.execute_reply.started":"2024-09-11T09:16:26.744225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["512\n"]}],"source":["length=len(dataset[0]['input_ids'])\n","print(length)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:16:50.975084Z","iopub.status.busy":"2024-09-11T09:16:50.974471Z","iopub.status.idle":"2024-09-11T09:16:50.979337Z","shell.execute_reply":"2024-09-11T09:16:50.978351Z","shell.execute_reply.started":"2024-09-11T09:16:50.975043Z"},"trusted":true},"outputs":[],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:26:57.361575Z","iopub.status.busy":"2024-09-11T09:26:57.361211Z","iopub.status.idle":"2024-09-11T09:26:57.370133Z","shell.execute_reply":"2024-09-11T09:26:57.369152Z","shell.execute_reply.started":"2024-09-11T09:26:57.361543Z"},"trusted":true},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","import torch\n","\n","class CustomDataCollator(DataCollatorForLanguageModeling):\n","    def __call__(self, features):\n","        batch = super().__call__(features)\n","        \n","        # Add custom ids to the batch, if they exist\n","        custom_fields = ['input_ids', 'segment_ids', 'emotion_ids']\n","        for field in custom_fields:\n","            if all(field in f for f in features):\n","                # Convert to tensor if it's a list\n","                field_data = [f[field] for f in features]\n","                if isinstance(field_data[0], list):\n","                    batch[field] = torch.tensor(field_data)\n","                else:\n","                    batch[field] = torch.stack(field_data)\n","            else:\n","                print(f\"Warning: '{field}' not found in all features. Skipping this field.\")\n","        \n","        return batch\n","\n","# Initialize the custom data collator\n","data_collator = CustomDataCollator(tokenizer=tokenizer, mlm=False)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:27:00.013536Z","iopub.status.busy":"2024-09-11T09:27:00.013148Z","iopub.status.idle":"2024-09-11T09:27:00.258078Z","shell.execute_reply":"2024-09-11T09:27:00.257275Z","shell.execute_reply.started":"2024-09-11T09:27:00.013499Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["trainer = SFTTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=dataset,\n","    max_seq_length=max_seq_length,\n","    dataset_num_proc=2,\n","    packing=True,\n","    args=TrainingArguments(\n","            per_device_train_batch_size = 2,\n","            gradient_accumulation_steps = 4,\n","            warmup_steps = 5,\n","            max_steps = 60,\n","            learning_rate = 2e-4,\n","            fp16 = not is_bfloat16_supported(),\n","            bf16 = is_bfloat16_supported(),\n","            logging_steps = 1,\n","            optim = \"adamw_8bit\",\n","            weight_decay = 0.01,\n","            lr_scheduler_type = \"linear\",\n","            seed = 3407,\n","            output_dir = \"outputs\",\n","        # ... (keep the existing arguments)\n","    ),\n","    data_collator=data_collator, \n",")"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T09:27:13.505470Z","iopub.status.busy":"2024-09-11T09:27:13.504658Z","iopub.status.idle":"2024-09-11T09:28:54.982855Z","shell.execute_reply":"2024-09-11T09:28:54.981103Z","shell.execute_reply.started":"2024-09-11T09:27:13.505434Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"696b6640c3f840debfe865fbaef45c68","version_major":2,"version_minor":0},"text/plain":["Counting untrained tokens:   0%|          | 0/19533 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Unsloth: Setting embed_tokens & lm_head untrained tokens to mean(trained) to counteract NaNs during training.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 19,533 | Num Epochs = 1\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 8 | Total steps = 60\n"," \"-____-\"     Number of trainable parameters = 1,060,110,336\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.9 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240911_092801-2lbdobdt</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/9861308106d-everest-engineering-college/huggingface/runs/2lbdobdt' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/9861308106d-everest-engineering-college/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/9861308106d-everest-engineering-college/huggingface' target=\"_blank\">https://wandb.ai/9861308106d-everest-engineering-college/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/9861308106d-everest-engineering-college/huggingface/runs/2lbdobdt' target=\"_blank\">https://wandb.ai/9861308106d-everest-engineering-college/huggingface/runs/2lbdobdt</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 3/60 00:13 < 12:56, 0.07 it/s, Epoch 0.00/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>4.292000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n","Warning: 'segment_ids' not found in all features. Skipping this field.\n","Warning: 'emotion_ids' not found in all features. Skipping this field.\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 252.00 MiB. GPU ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<string>:145\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n","File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3328\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3328\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3333\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3334\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3373\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3372\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3373\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3374\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3375\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:1032\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1031\u001b[0m ):\n\u001b[0;32m-> 1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:959\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states[:, \u001b[38;5;241m-\u001b[39mnum_logits_to_keep:, :]\u001b[38;5;241m.\u001b[39mto(lm_head\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 959\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    961\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtorch_dtype)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:264\u001b[0m, in \u001b[0;36mModulesToSaveWrapper.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_adapters \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_save):\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_module(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules_to_save\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 252.00 MiB. GPU "]}],"source":["# Run the training\n","trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
