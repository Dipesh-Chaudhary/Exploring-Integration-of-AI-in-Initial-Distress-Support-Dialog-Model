{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==2.3.0 torchvision torchaudio -f https://download.pytorch.org/whl/cu121/torch_stable.html","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:30:08.903122Z","iopub.execute_input":"2024-07-01T13:30:08.903840Z","iopub.status.idle":"2024-07-01T13:34:58.194016Z","shell.execute_reply.started":"2024-07-01T13:30:08.903808Z","shell.execute_reply":"2024-07-01T13:34:58.193110Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: https://download.pytorch.org/whl/cu121/torch_stable.html\nCollecting torch==2.3.0\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (2024.3.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch==2.3.0)\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0) (1.3.0)\nDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:02\u001b[0mm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 triton-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check Python version\nimport sys\nprint(\"Python version:\", sys.version)\n\n# Check PyTorch version\nimport torch\nprint(\"PyTorch version:\", torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:34:58.196589Z","iopub.execute_input":"2024-07-01T13:34:58.197068Z","iopub.status.idle":"2024-07-01T13:35:00.105364Z","shell.execute_reply.started":"2024-07-01T13:34:58.197031Z","shell.execute_reply":"2024-07-01T13:35:00.104442Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Python version: 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\nPyTorch version: 2.3.0+cu121\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:35:00.106353Z","iopub.execute_input":"2024-07-01T13:35:00.106705Z","iopub.status.idle":"2024-07-01T13:35:45.030618Z","shell.execute_reply.started":"2024-07-01T13:35:00.106681Z","shell.execute_reply":"2024-07-01T13:35:45.029475Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-7yba165x/unsloth_cd11dcff676d40dd82dfc7bc16761ff6\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-7yba165x/unsloth_cd11dcff676d40dd82dfc7bc16761ff6\n  Resolved https://github.com/unslothai/unsloth.git to commit 933d9fe2cb2459f949ee2250e90a5b610d277eab\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.3.0+cu121)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\nCollecting xformers@ https://download.pytorch.org/whl/cu121/xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: transformers>=4.38.2 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.41.2)\nRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\nRequirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (5.9.3)\nRequirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.42.0)\nRequirement already satisfied: accelerate>=0.26.1 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.30.1)\nCollecting trl<0.9.0,>=0.7.9 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\nCollecting peft!=0.11.0,>=0.7.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.9.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (12.5.40)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\nCollecting docstring-parser>=0.16 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch230]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.6-py3-none-any.whl size=117332 sha256=fa7239fa26059c3dccd79352315484fb3a582e66cccc64d42106874791ae3872\n  Stored in directory: /tmp/pip-ephem-wheel-cache-2_k1eic3/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\nSuccessfully built unsloth\nInstalling collected packages: unsloth, shtab, docstring-parser, tyro, xformers, bitsandbytes, trl, peft\n  Attempting uninstall: docstring-parser\n    Found existing installation: docstring-parser 0.15\n    Uninstalling docstring-parser-0.15:\n      Successfully uninstalled docstring-parser-0.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.43.1 docstring-parser-0.16 peft-0.11.1 shtab-1.7.1 trl-0.8.6 tyro-0.8.5 unsloth-2024.6 xformers-0.0.26.post1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install triton","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:35:45.033724Z","iopub.execute_input":"2024-07-01T13:35:45.034552Z","iopub.status.idle":"2024-07-01T13:35:58.049566Z","shell.execute_reply.started":"2024-07-01T13:35:45.034516Z","shell.execute_reply":"2024-07-01T13:35:58.048401Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: triton in /opt/conda/lib/python3.10/site-packages (2.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton) (3.13.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:35:58.051091Z","iopub.execute_input":"2024-07-01T13:35:58.051419Z","iopub.status.idle":"2024-07-01T13:36:19.191117Z","shell.execute_reply.started":"2024-07-01T13:35:58.051393Z","shell.execute_reply":"2024-07-01T13:36:19.190154Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-07-01 13:36:07.471253: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-01 13:36:07.471405: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-01 13:36:07.630540: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:36:19.192402Z","iopub.execute_input":"2024-07-01T13:36:19.193110Z","iopub.status.idle":"2024-07-01T13:36:19.199163Z","shell.execute_reply.started":"2024-07-01T13:36:19.193074Z","shell.execute_reply":"2024-07-01T13:36:19.198342Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:36:19.200340Z","iopub.execute_input":"2024-07-01T13:36:19.201358Z","iopub.status.idle":"2024-07-01T13:37:01.381129Z","shell.execute_reply.started":"2024-07-01T13:36:19.201302Z","shell.execute_reply":"2024-07-01T13:37:01.380176Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b94feddf90e84028928f46e5def2e84c"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.6\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5469ecf7f4b4c93975cbc2f51297a38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1df6eceec30842588dede0cebb658774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"078517664a8f418287d0569fe6c224aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f31b4475c074c97aff724003d30eeb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c1b0b6d01894c778cdb2dc99ecd2dd7"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We now add LoRA adapters so we only need to update 1 to 10% of all parameters!","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:01.382258Z","iopub.execute_input":"2024-07-01T13:37:01.382593Z","iopub.status.idle":"2024-07-01T13:37:06.210159Z","shell.execute_reply.started":"2024-07-01T13:37:01.382569Z","shell.execute_reply":"2024-07-01T13:37:06.209367Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**DATA PREP**","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\npass","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:06.211408Z","iopub.execute_input":"2024-07-01T13:37:06.211712Z","iopub.status.idle":"2024-07-01T13:37:08.512232Z","shell.execute_reply.started":"2024-07-01T13:37:06.211687Z","shell.execute_reply":"2024-07-01T13:37:08.511181Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n# dataset = load_dataset(\"nbertagnolli/counsel-chat\", split = \"train\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:08.516007Z","iopub.execute_input":"2024-07-01T13:37:08.516289Z","iopub.status.idle":"2024-07-01T13:37:08.595757Z","shell.execute_reply.started":"2024-07-01T13:37:08.516267Z","shell.execute_reply":"2024-07-01T13:37:08.594802Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now. let's change dataset into ShareGPT style","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\ndef transform_to_sharegpt(example):\n    return {\n        \"conversations\": [\n            {\"from\": \"human\", \"value\": example[\"questionText\"]},\n            {\"from\": \"gpt\", \"value\": example[\"answerText\"]}\n        ]\n    }\n\n# # Assuming you have already loaded your dataset\n# original_dataset = Dataset.from_dict(dataset['train'])\n\n# Apply the transformation\nsharegpt_dataset = load_dataset(\"nbertagnolli/counsel-chat\", split = \"train\").map(transform_to_sharegpt, remove_columns=load_dataset(\"nbertagnolli/counsel-chat\", split = \"train\").column_names)\n\n# The resulting dataset will have a single column 'conversations' with the desired format\nprint(sharegpt_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:08.596982Z","iopub.execute_input":"2024-07-01T13:37:08.597343Z","iopub.status.idle":"2024-07-01T13:37:12.293772Z","shell.execute_reply.started":"2024-07-01T13:37:08.597295Z","shell.execute_reply":"2024-07-01T13:37:12.292822Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff6b27462e140a992a979b7987649e7"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa8eae61798f4d84962e348e9666a913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2775 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e02f45dbd4404d208165deb89ca98821"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2775 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83b180d946749b39550db2a47d0aa42"}},"metadata":{}},{"name":"stdout","text":"{'conversations': [{'from': 'human', 'value': 'I have so many issues to address. I have a history of sexual abuse, Iâ€™m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and Iâ€™m beginning to have anxiety. I have low self esteem but Iâ€™ve been happily married for almost 35 years.\\n   Iâ€™ve never had counseling about any of this. Do I have too many issues to address in counseling?'}, {'from': 'gpt', 'value': 'It is very common for\\xa0people to have multiple issues that they want to (and need to) address in counseling.\\xa0 I have had clients ask that same question and through more exploration, there is often an underlying fear that they\\xa0 \"can\\'t be helped\" or that they will \"be too much for their therapist.\" I don\\'t know if any of this rings true for you. But, most people have more than one problem in their lives and more often than not,\\xa0 people have numerous significant stressors in their lives.\\xa0 Let\\'s face it, life can be complicated! Therapists are completely ready and equipped to handle all of the issues small or large that a client presents in session. Most therapists over the first couple of sessions will help you prioritize the issues you are facing so that you start addressing the issues that are causing you the most distress.\\xa0 You can never have too many issues to address in counseling.\\xa0 All of the issues you mention above can be successfully worked through in counseling.'}]}\n","output_type":"stream"}]},{"cell_type":"code","source":"sharegpt_dataset = sharegpt_dataset.map(formatting_prompts_func, batched = True,)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:12.295205Z","iopub.execute_input":"2024-07-01T13:37:12.295893Z","iopub.status.idle":"2024-07-01T13:37:12.776475Z","shell.execute_reply.started":"2024-07-01T13:37:12.295859Z","shell.execute_reply":"2024-07-01T13:37:12.775630Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2775 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16531f6bea1447b6a3e2f645ae1e4b7d"}},"metadata":{}}]},{"cell_type":"code","source":"sharegpt_dataset[0][\"conversations\"]","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:12.777797Z","iopub.execute_input":"2024-07-01T13:37:12.778631Z","iopub.status.idle":"2024-07-01T13:37:12.787003Z","shell.execute_reply.started":"2024-07-01T13:37:12.778594Z","shell.execute_reply":"2024-07-01T13:37:12.786128Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[{'from': 'human',\n  'value': 'I have so many issues to address. I have a history of sexual abuse, Iâ€™m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and Iâ€™m beginning to have anxiety. I have low self esteem but Iâ€™ve been happily married for almost 35 years.\\n   Iâ€™ve never had counseling about any of this. Do I have too many issues to address in counseling?'},\n {'from': 'gpt',\n  'value': 'It is very common for\\xa0people to have multiple issues that they want to (and need to) address in counseling.\\xa0 I have had clients ask that same question and through more exploration, there is often an underlying fear that they\\xa0 \"can\\'t be helped\" or that they will \"be too much for their therapist.\" I don\\'t know if any of this rings true for you. But, most people have more than one problem in their lives and more often than not,\\xa0 people have numerous significant stressors in their lives.\\xa0 Let\\'s face it, life can be complicated! Therapists are completely ready and equipped to handle all of the issues small or large that a client presents in session. Most therapists over the first couple of sessions will help you prioritize the issues you are facing so that you start addressing the issues that are causing you the most distress.\\xa0 You can never have too many issues to address in counseling.\\xa0 All of the issues you mention above can be successfully worked through in counseling.'}]"},"metadata":{}}]},{"cell_type":"code","source":"print(sharegpt_dataset[0][\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:12.788087Z","iopub.execute_input":"2024-07-01T13:37:12.788370Z","iopub.status.idle":"2024-07-01T13:37:12.804256Z","shell.execute_reply.started":"2024-07-01T13:37:12.788345Z","shell.execute_reply":"2024-07-01T13:37:12.803382Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nI have so many issues to address. I have a history of sexual abuse, Iâ€™m a breast cancer survivor and I am a lifetime insomniac.    I have a long history of depression and Iâ€™m beginning to have anxiety. I have low self esteem but Iâ€™ve been happily married for almost 35 years.\n   Iâ€™ve never had counseling about any of this. Do I have too many issues to address in counseling?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nIt is very common forÂ people to have multiple issues that they want to (and need to) address in counseling.Â  I have had clients ask that same question and through more exploration, there is often an underlying fear that theyÂ  \"can't be helped\" or that they will \"be too much for their therapist.\" I don't know if any of this rings true for you. But, most people have more than one problem in their lives and more often than not,Â  people have numerous significant stressors in their lives.Â  Let's face it, life can be complicated! Therapists are completely ready and equipped to handle all of the issues small or large that a client presents in session. Most therapists over the first couple of sessions will help you prioritize the issues you are facing so that you start addressing the issues that are causing you the most distress.Â  You can never have too many issues to address in counseling.Â  All of the issues you mention above can be successfully worked through in counseling.<|eot_id|>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**TRAIN THE MODEL**","metadata":{}},{"cell_type":"markdown","source":"Now let's use Huggingface TRL's SFTTrainer! We do 60 steps to speed things up, but you can set num_train_epochs=1 for a full run, and turn off max_steps=None. We also support TRL's DPOTrainer!","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = sharegpt_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:12.805277Z","iopub.execute_input":"2024-07-01T13:37:12.805650Z","iopub.status.idle":"2024-07-01T13:37:16.402992Z","shell.execute_reply.started":"2024-07-01T13:37:12.805625Z","shell.execute_reply":"2024-07-01T13:37:16.402065Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2775 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccb1b64f6160428d9ce0024f00753304"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:16.404360Z","iopub.execute_input":"2024-07-01T13:37:16.404648Z","iopub.status.idle":"2024-07-01T13:37:16.411088Z","shell.execute_reply.started":"2024-07-01T13:37:16.404618Z","shell.execute_reply":"2024-07-01T13:37:16.410092Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.748 GB.\n5.594 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:37:16.412272Z","iopub.execute_input":"2024-07-01T13:37:16.412637Z","iopub.status.idle":"2024-07-01T13:47:25.819746Z","shell.execute_reply.started":"2024-07-01T13:37:16.412612Z","shell.execute_reply":"2024-07-01T13:47:25.818548Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 2,775 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 41,943,040\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240701_133740-y9oiu5fc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface/runs/y9oiu5fc' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface' target=\"_blank\">https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface/runs/y9oiu5fc' target=\"_blank\">https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface/runs/y9oiu5fc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 09:14, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.853000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.128400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.080800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.860300</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.918400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.657500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.568300</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.575200</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.424800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.379800</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.510600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.467500</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.427200</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.519900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.455300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.626500</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.301600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>2.528500</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.115600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.406600</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.353500</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>2.288200</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>2.373800</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.479000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.454700</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>2.452100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.358600</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.352200</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>2.420400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.515500</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>2.307800</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>2.264800</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>2.411900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.353100</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.316900</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>2.062100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>2.316400</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>2.288700</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>2.412200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.233900</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>2.415700</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>2.257300</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>2.266100</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>2.340600</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>2.174200</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>2.130700</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>2.471400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>2.383600</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>2.377600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.452600</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>2.453900</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>2.176600</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>2.471800</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>2.299800</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>2.376100</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>2.266500</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>2.385100</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>2.377700</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>2.458000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.275200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"\nmodel.push_to_hub(\"DipeshChaudhary/ShareGPTChatBot-Counselchat1\", token = \"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\") # Online saving","metadata":{"execution":{"iopub.status.busy":"2024-07-01T13:53:14.440573Z","iopub.execute_input":"2024-07-01T13:53:14.440893Z","iopub.status.idle":"2024-07-01T13:53:23.304456Z","shell.execute_reply.started":"2024-07-01T13:53:14.440868Z","shell.execute_reply":"2024-07-01T13:53:23.303388Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/600 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbcc7d4d79684b81afeed7bc91cc8e6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce6fa9be34f41ad80d28120263566eb"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/DipeshChaudhary/ShareGPTChatBot-Counselchat1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.makedirs(\"/kaggle/working/ShareGPTChatBot-Counselchat1\", exist_ok=True)\nmodel.save_pretrained(\"/kaggle/working/ShareGPTChatBot-Counselchat1\") # Local saving","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:00:17.834399Z","iopub.execute_input":"2024-07-01T14:00:17.835255Z","iopub.status.idle":"2024-07-01T14:00:19.017183Z","shell.execute_reply.started":"2024-07-01T14:00:17.835222Z","shell.execute_reply":"2024-07-01T14:00:19.016033Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"from\": \"human\", \"value\": \"I'm worry about my exam.\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:17:33.295354Z","iopub.execute_input":"2024-07-01T14:17:33.296069Z","iopub.status.idle":"2024-07-01T14:17:41.389613Z","shell.execute_reply.started":"2024-07-01T14:17:33.296030Z","shell.execute_reply":"2024-07-01T14:17:41.388699Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nI'm worry about my exam.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nI'm sorry to hear that you're worried about your exam.Â  It's normal to feel worried about an exam, especially if you're not sure how you'll do.Â  If you're feeling worried, you might want to talk to your teacher or professor about your concerns.Â  They may be able to give you some advice or help you feel more prepared for the exam.Â  If you're feeling overwhelmed, you might want to talk to a counselor.Â  They can help you feel more prepared and give you some tips on how to manage your anxiety.<|eot_id|>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to generate response\ndef generate_response(input_text):\n    messages = [{\"from\": \"human\", \"value\": input_text}]\n    inputs = tokenizer.apply_chat_template(messages,\n                                           tokenize = True,\n                                           add_generation_prompt = True, # Must add for generation\n                                           return_tensors = \"pt\",\n                                          ).to(\"cuda\")\n    text_streamer = TextStreamer(tokenizer) \n    return model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)\n\n# Example usage\nconversation_history = []\nwhile True:\n    user_input = input(\"User: \")\n    if user_input.lower() == \"exit\":\n        print(\"Exiting...\")\n        break\n    \n    # Append user message to history\n    conversation_history.append({\"from\": \"human\", \"value\": user_input})\n    \n    # Generate response\n    response = generate_response(user_input)\n    \n    # Append bot response to history\n    conversation_history.append({\"from\": \"bot\", \"value\": response})\n    \n    # Print bot's response\n#     print(\"Bot:\", response)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:15:25.196750Z","iopub.execute_input":"2024-07-01T14:15:25.197112Z","iopub.status.idle":"2024-07-01T14:17:33.293172Z","shell.execute_reply.started":"2024-07-01T14:15:25.197084Z","shell.execute_reply":"2024-07-01T14:17:33.292078Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdin","text":"User:  hlo\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nhlo<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHi!<|eot_id|>\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  can i help you?\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\ncan i help you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nI'm not sure what you mean by \"help me.\" I'm a computer program, so I don't have the ability to help you in the way that a human would. I can provide you with information, answer questions, and help you to find the information you need.<|eot_id|>\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  what do you mean by c++\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nwhat do you mean by c++<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nC++ is a programming language that is used to create applications that run on a computer. It is a general-purpose language that is used to create a wide range of applications, from simple programs to complex systems.<|eot_id|>\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  i need your help\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\ni need your help<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nI'm here to help. What's going on?<|eot_id|>\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  exit\n"},{"name":"stdout","text":"Exiting...\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}